"use strict";(self.webpackChunkvisdecode_site=self.webpackChunkvisdecode_site||[]).push([[4282],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>u});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),d=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=d(e.components);return a.createElement(l.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(n),h=i,u=p["".concat(l,".").concat(h)]||p[h]||m[h]||o;return n?a.createElement(u,r(r({ref:t},c),{},{components:n})):a.createElement(u,r({ref:t},c))}));function u(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:i,r[1]=s;for(var d=2;d<o;d++)r[d]=n[d];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},6969:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var a=n(7462),i=(n(7294),n(3905));const o={slug:"exploring-data-science-job-trends-python-react-d3",title:"Exploring Data Science Job Trends in the US: A Python and React Data Analysis Journey- Part 1",authors:["mehdi"],tags:["topic modeling","eda","python","dashboard","data analysis"],hide_table_of_contents:!1,toc_min_heading_level:2,toc_max_heading_level:5},r=void 0,s={permalink:"/visualdecode/blog/exploring-data-science-job-trends-python-react-d3",source:"@site/blog/2023-03-10-eda-dashboard.md",title:"Exploring Data Science Job Trends in the US: A Python and React Data Analysis Journey- Part 1",description:"Exploring job trends in the data science industry can reveal insights into the skills and experience employers are seeking, and help job seekers identify promising career opportunities. In this blog post, we will take a deep dive into a dataset of data science jobs in the US, exploring trends in job titles, companies, and job announcement platforms. We will also use topic modeling to extract insights from job titles. Finally, we will build an interactive dashboard with React and D3 to visualize our findings in a way that allows users to explore and analyze the data in real-time.",date:"2023-03-10T00:00:00.000Z",formattedDate:"March 10, 2023",tags:[{label:"topic modeling",permalink:"/visualdecode/blog/tags/topic-modeling"},{label:"eda",permalink:"/visualdecode/blog/tags/eda"},{label:"python",permalink:"/visualdecode/blog/tags/python"},{label:"dashboard",permalink:"/visualdecode/blog/tags/dashboard"},{label:"data analysis",permalink:"/visualdecode/blog/tags/data-analysis"}],readingTime:18.015,hasTruncateMarker:!0,authors:[{name:"Mehdi Allahyari",title:"Principle Research Scientist",url:"https://github.com/mallahyari",imageURL:"https://github.com/mallahyari.png",key:"mehdi"}],frontMatter:{slug:"exploring-data-science-job-trends-python-react-d3",title:"Exploring Data Science Job Trends in the US: A Python and React Data Analysis Journey- Part 1",authors:["mehdi"],tags:["topic modeling","eda","python","dashboard","data analysis"],hide_table_of_contents:!1,toc_min_heading_level:2,toc_max_heading_level:5},nextItem:{title:"Exploring Data Distributions with an Interactive Ridge Plot",permalink:"/visualdecode/blog/exploring-data-distributions-with-interactive-ridge-plot"}},l={authorsImageUrls:[void 0]},d=[{value:"Collecting and Preparing the Data",id:"collecting-and-preparing-the-data",level:2},{value:"Exploratory Data Analysis (EDA)",id:"exploratory-data-analysis-eda",level:2},{value:"Most common words in job titles",id:"most-common-words-in-job-titles",level:3},{value:"N-gram Analysis",id:"n-gram-analysis",level:3},{value:"Identify Seniority Level",id:"identify-seniority-level",level:3},{value:"Topic modeling on Job Titles",id:"topic-modeling-on-job-titles",level:2}],c={toc:d};function p(e){let{components:t,...o}=e;return(0,i.kt)("wrapper",(0,a.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Exploring job trends in the data science industry can reveal insights into the skills and experience employers are seeking, and help job seekers identify promising career opportunities. In this blog post, we will take a deep dive into a dataset of data science jobs in the US, exploring trends in job titles, companies, and job announcement platforms. We will also use topic modeling to extract insights from job titles. Finally, we will build an interactive dashboard with React and D3 to visualize our findings in a way that allows users to explore and analyze the data in real-time."),(0,i.kt)("p",null,"Nevertheless, I decided to split this tutorial into two parts: In the first part we will do all the data analysis and exploration in Python. And we will create an interactive dashboard to work with the processed data in the second part of this tutorial."),(0,i.kt)("p",null,"Our goal is to provide a comprehensive guide to exploring and analyzing data science job trends, using real-world data and techniques. We chose this topic because the demand for data scientists is growing rapidly, and understanding the job market is essential for both job seekers and employers. We will begin by collecting and preparing the data, using a dataset that we feel is representative of the industry. We will then perform exploratory data analysis using Python, identifying trends in job titles, companies, and job announcement platforms. Next, we will use topic modeling to extract insights from job titles, looking for common themes and skills that are in high demand."),(0,i.kt)("p",null,"Finally, we will build an interactive dashboard with React and D3 to visualize our findings in a way that allows users to explore and analyze the data in real-time. We will showcase multiple visualizations, such as bar charts, heatmaps, and word clouds, that will be linked together to provide a cohesive and intuitive experience."),(0,i.kt)("p",null,"We believe that this approach will provide readers with a comprehensive guide to exploring and analyzing data science job trends. By combining real-world data with industry-standard tools and techniques, we hope to empower readers to gain insights into the job market, identify promising career opportunities, and make data-driven decisions."),(0,i.kt)("h2",{id:"collecting-and-preparing-the-data"},"Collecting and Preparing the Data"),(0,i.kt)("p",null,"Before we can begin our analysis, we need to collect and prepare our data. We are going to use a dataset from Kaggle, ",(0,i.kt)("a",{parentName:"p",href:"https://www.kaggle.com/datasets/diegosilvadefrana/2023-data-scientists-jobs-descriptions"},"2023 Data Scientist Job Descriptions"),". It's a very recent dataset, which makes it even more interesting. We download the file and save it locally for easy access."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Read and load the file\ndf = pd.read_csv('ds_jobs.csv')\n")),(0,i.kt)("p",null,"Image below shows a few rows of this dataset.\n",(0,i.kt)("img",{alt:"dataset dataframe snippet",src:n(7745).Z,width:"965",height:"191"})),(0,i.kt)("p",null,"Once we have our dataset, we need to clean it up to make sure it is consistent and usable. This may involve removing duplicates, correcting errors, filling in missing values, and converting data types as needed. Although, it's already mentioned on Kaggle that this dataset has no missing value, let's take a look and make sure of it."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Get statistics about dataframe columns\ndf.describe(include='all')\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"df statistics",src:n(4350).Z,width:"716",height:"144"})),(0,i.kt)("p",null,"As the image shows there is no missing values in any of the columns. Depending on the type of analysis we're interested in, we may need to further pre-process our data like removing stop words, punctuation and special characters, lowercasing the text, etc. Next step is to define what kinds of analyses we want to do."),(0,i.kt)("h2",{id:"exploratory-data-analysis-eda"},"Exploratory Data Analysis (EDA)"),(0,i.kt)("p",null,"As a data scientist, here are ",(0,i.kt)("em",{parentName:"p"},"some")," interesting analyses that can be run on this dataset:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Top Companies: Analyze the dataset to find out which companies are hiring the most for data science-related jobs."),(0,i.kt)("li",{parentName:"ol"},"Most Popular Job Titles: Determine which job titles are most frequently advertised in the dataset."),(0,i.kt)("li",{parentName:"ol"},"Announcements Distribution: Explore the distribution of job postings across different job announcement platforms."),(0,i.kt)("li",{parentName:"ol"},"Job Descriptions Analysis: Analyze the job descriptions to identify the most common technical skills or qualifications required for data science roles."),(0,i.kt)("li",{parentName:"ol"},"Job Level Analysis: Categorize the job titles in the dataset by level (e.g., junior, senior, manager) and analyze the distribution of job levels."),(0,i.kt)("li",{parentName:"ol"},"Text Analysis: Perform text analysis on job titles and job descriptions to identify the most common words or phrases used in the data science job market."),(0,i.kt)("li",{parentName:"ol"},"Correlation between job title and required level of education or experience for data science positions."),(0,i.kt)("li",{parentName:"ol"},"Word Cloud: Generate a word cloud to visualize the most common words in the job titles."),(0,i.kt)("li",{parentName:"ol"},"Job Title Similarity: Use clustering algorithms to group similar job titles together and identify any patterns in the way companies name their job titles."),(0,i.kt)("li",{parentName:"ol"},"Topic modeling on job titles: To gain insights into the most common themes and subtopics that are present in data science job titles"),(0,i.kt)("li",{parentName:"ol"},"Average salaries for data science positions by job title and/or company. This analysis can provide insights into the earning potential of different data science roles and can help job seekers negotiate salaries and benefits.")),(0,i.kt)("p",null,"We are going to work on some of them, and the remaining ones can be a good exercise for you. \ud83d\ude42"),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"For visualizations in python I will be using ",(0,i.kt)("a",{parentName:"p",href:"https://docs.bokeh.org/en/latest/"},"Bokeh"),"."),(0,i.kt)("p",{parentName:"admonition"},(0,i.kt)("em",{parentName:"p"},'"Bokeh is a Python library for creating interactive visualizations for modern web browsers. It helps you build beautiful graphics, ranging from simple plots to complex dashboards with streaming datasets. With Bokeh, you can create JavaScript-powered visualizations without writing any JavaScript yourself."'))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"What is the distribution of hiring companies?"),"\nTo answer this question, we need to have the number of job posting for each company."),(0,i.kt)("p",null,"Top ten companies having the most job ads are displayed below, and surprisingly it's not any of the FAANG companies. ",(0,i.kt)("em",{parentName:"p"},"Of course it's because our dataset is very small.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"Upwork                              13\nWalmart                             12\nDice                                10\nBooz Allen Hamilton                 10\nCardinal Health                      6\nSynergisticIT                        5\nIntermountain Healthcare             4\nStaffigo Technical Services, LLC     4\nStaffigo                             4\nGuidehouse                           4\n")),(0,i.kt)("p",null,"Here's the code that extracts job posting for companies and creates a line chart showing the distribution of jobs over companies."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Extract companies (665 companies)\ncompanies = df['company'].value_counts().rename_axis('company').reset_index(name='counts')\n\nsource = ColumnDataSource(data=companies)\n\n# Create line chart\np = figure(height=350, width=600, title=f'Distribution of hiring companies', tools=\"\")\np.line(x='index', y='counts', line_color='red', source=source, line_width=2)\n\n# rotate labels by 45 degrees\np.xaxis.major_label_orientation = 3.14/4\nticks = [0, 20, 40, 100, 150, 200, len(companies) - 1]\np.xaxis.ticker = ticks\nxlabel_ticks = {}\nfor t in ticks:\n    xlabel_ticks[t] = companies['company'][t]\np.xaxis.major_label_overrides = xlabel_ticks\np.xaxis.axis_label = 'Companies'\np.yaxis.axis_label = 'Counts'\np.xgrid.grid_line_color = None\n\nshow(p)\n")),(0,i.kt)("p",null,"Because there are 665 companies, I only show several of them on the x-axis to prevent cluttering the plot.\n",(0,i.kt)("img",{alt:"eda-company-distribution",src:n(6349).Z,width:"600",height:"350"})),(0,i.kt)("p",null,"Except for the first few companies, others only have posted one job according to our dataset."),(0,i.kt)("p",null,"Using almost the same code with slight change, we can see ",(0,i.kt)("strong",{parentName:"p"},"the distribution of job postings across different job announcement platforms.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Extract job announcements\njob_announcements = df['announcement'].value_counts().rename_axis('announcement').reset_index(name='counts')\n\nsource = ColumnDataSource(data=job_announcements)\n\n# Create line chart\np = figure(height=350, width=600, title=f'Distribution of job announcement platforms')\np.line(x='index', y='counts', line_color='red', source=source, line_width=2)\n\n# rotate labels by 45 degrees\np.xaxis.major_label_orientation = 3.14/4\nticks = [0, 20, 40, 100, 150, 200]\np.xaxis.ticker = ticks\nxlabel_ticks = {}\nfor t in ticks:\n    xlabel_ticks[t] = job_announcements['announcement'][t]\np.xaxis.major_label_overrides = xlabel_ticks\np.xaxis.axis_label = 'Platforms'\np.yaxis.axis_label = 'Counts'\np.xgrid.grid_line_color = None\n\nshow(p)\n")),(0,i.kt)("p",null,"As the image and top ten rows show, ",(0,i.kt)("em",{parentName:"p"},"LinkedIn")," is the number one platform for job posting.\n",(0,i.kt)("img",{alt:"eda-company-distribution",src:n(6953).Z,width:"600",height:"350"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Top-10 platforms with the most number of job postings.\nLinkedIn        189\nSimplyHired      79\nZipRecruiter     66\nSalary.com       41\nStartup Jobs     20\nAdzuna           20\nGlassdoor        20\nGreenhouse       14\nUpwork           13\nBuilt In         13\n")),(0,i.kt)("h3",{id:"most-common-words-in-job-titles"},"Most common words in job titles"),(0,i.kt)("p",null,"To find the most common words in job titles, we need:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Get the list of all job titles"),(0,i.kt)("li",{parentName:"ul"},"Clean titles"),(0,i.kt)("li",{parentName:"ul"},"Tokenize titles"),(0,i.kt)("li",{parentName:"ul"},"Remove stop words"),(0,i.kt)("li",{parentName:"ul"},"Calculate word-frequency"),(0,i.kt)("li",{parentName:"ul"},"Prepare data for visualization"),(0,i.kt)("li",{parentName:"ul"},"Draw the plot")),(0,i.kt)("p",null,"The following code does all the steps, please make sure to read the comments."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk import ngrams\nimport string\nimport re\n\n\n# Extract job titles\njob_titles = df['title'].tolist()\n\n# Preprocessing\nstop_words = set(stopwords.words('english'))\ntokens = []\nprocessed_titles = []\nfor title in job_titles:\n    # Convert to lowercase\n    title = title.lower()\n    # Remove punctuation and special characters\n    title = re.sub(r'[^\\w\\s]', '', title)\n    # Tokenize title\n    title_tokens = word_tokenize(title)\n    # Remove stop words\n    title_tokens = [token for token in title_tokens if token not in stop_words]\n    tokens.extend(title_tokens)\n    processed_titles.append(title_tokens)\n\n\n# Calculate word frequency\nfdist = FreqDist(tokens)\ntop_n = 20\ntop_words = dict(fdist.most_common(top_n))\n\n# Prepare data for visualization\ndata = {'words': list(top_words.keys()), 'frequency': list(top_words.values())}\nsource = ColumnDataSource(data=data)\n\n# Create bar chart\np = figure(x_range=data['words'], height=350, title=f'Top {top_n} Most Common Words in Job Titles', toolbar_location=None, tools=\"\")\np.line(x='words', y='frequency', line_color='red', source=source, line_width=2)\np.xaxis.major_label_orientation = 3.14/4 # rotate labels by 45 degrees\np.xgrid.grid_line_color = None\n\nshow(p)\n\n")),(0,i.kt)("p",null,"By changing the ",(0,i.kt)("inlineCode",{parentName:"p"},"top_n")," variable, you can see more/less common words."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"top-20 common words",src:n(7136).Z,width:"600",height:"350"})),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"data"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"analyst"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"scientist"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"senior")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"analytics")," have the highest frequencies."),(0,i.kt)("p",null,"Let's generate the word cloud for that:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from wordcloud import WordCloud\n\n# Generate word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(fdist)\n\n# Visualize word cloud\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n# Save to a file\nwordcloud.to_file('top20-wordcloud.png')\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"top-20 common words word cloud",src:n(6633).Z,width:"800",height:"400"})),(0,i.kt)("h3",{id:"n-gram-analysis"},"N-gram Analysis"),(0,i.kt)("p",null,"We are going to analyze n-grams (i.e. bigrams and trigrams) in job titles to identify patterns or trends in job titles."),(0,i.kt)("p",null,"First let's see the distribution of titles' lengths:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Calculate length of job titles\ndf['title_length'] = df['title'].apply(lambda x: len(x.split()))\n\n# Create a histogram of title lengths\ntitle_lengths = df['title_length'].tolist()\nhist, edges = np.histogram(df['title_length'])\n\n# Shift to center the tick labels\nedges = edges - (edges[1] - edges[0]) / 2\n\n# Create the plot\np = figure(title='Title Length Distribution',\n           x_axis_label='Number of Words in Title',\n           y_axis_label='Frequency')\n\np.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color='white')\np.y_range.start = 0\n\n# Show the plot\nshow(p)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"top-20 common words word cloud",src:n(7389).Z,width:"600",height:"600"})),(0,i.kt)("p",null,"As the image shows, titles have a range of ","[2, 11]"," words, where most titles consist of less than 5 words. Now we'll calculate the bigrams and trigrams."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Calculate bigrams and trigrams\nbigrams = []\ntrigrams = []\nfor title in processed_titles:\n    bigrams.extend(list(ngrams(title, 2)))\n    trigrams.extend(list(ngrams(title, 3)))\n\n# Calculate frequency distribution of bigrams and trigrams\nbigram_fdist = FreqDist(bigrams)\ntrigram_fdist = FreqDist(trigrams)\n\n# Print the top 10 most common bigrams\nprint('Top 10 most common bigrams:')\nfor bigram, frequency in bigram_fdist.most_common(10):\n    print(bigram, frequency)\n\n# Print the top 10 most common trigrams\nprint('\\nTop 10 most common trigrams:')\nfor trigram, frequency in trigram_fdist.most_common(10):\n    print(trigram, frequency)\n")),(0,i.kt)("p",null,"Here's the output of the code:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"Top 10 most common bigrams:\n('data', 'analyst') 341\n('data', 'scientist') 277\n('senior', 'data') 108\n('data', 'science') 43\n('data', 'analytics') 35\n('sr', 'data') 27\n('business', 'data') 25\n('data', 'specialist') 23\n('lead', 'data') 20\n('data', 'analysis') 16\n\nTop 10 most common trigrams:\n('senior', 'data', 'scientist') 58\n('senior', 'data', 'analyst') 42\n('business', 'data', 'analyst') 20\n('sr', 'data', 'scientist') 16\n('sr', 'data', 'analyst') 11\n('staff', 'data', 'scientist') 9\n('lead', 'data', 'scientist') 9\n('lead', 'data', 'analyst') 9\n('entry', 'level', 'data') 8\n('level', 'data', 'analyst') 8\n")),(0,i.kt)("p",null,"Let's create bar chart for them:"),(0,i.kt)("details",null,(0,i.kt)("summary",null,"Show code"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap, factor_cmap\nfrom bokeh.palettes import Blues\n\n# Extract the top 10 most common bigrams and trigrams\ntop_bigrams = bigram_fdist.most_common(10)\ntop_trigrams = trigram_fdist.most_common(10)\n\n# Create a list of the bigram and trigram labels\nbigram_labels = [', '.join(bigram) for bigram, _ in top_bigrams]\ntrigram_labels = [', '.join(trigram) for trigram, _ in top_trigrams]\n\n# Create a list of the bigram and trigram frequencies\nbigram_frequencies = [frequency for _, frequency in top_bigrams]\ntrigram_frequencies = [frequency for _, frequency in top_trigrams]\n\n# Create a ColumnDataSource for the bigrams and trigrams\nbigram_source = ColumnDataSource(data=dict(labels=bigram_labels, frequencies=bigram_frequencies))\ntrigram_source = ColumnDataSource(data=dict(labels=trigram_labels, frequencies=trigram_frequencies))\n\n# Define a color map based on the height of the bars\n# color_mapper = linear_cmap(field_name='frequencies', palette=Blues[9], low=-100, high=max(bigram_frequencies))\ncolor_mapper = linear_cmap(field_name='frequencies', palette=Blues[9], low=min(bigram_frequencies), high=max(bigram_frequencies))\n\n\n# Create a figure for the bigrams\nbigram_plot = figure(y_range=bigram_labels, height=400, width=600,\n                     title='Top 10 most common bigrams in job titles')\nbigram_plot.hbar(y='labels', right='frequencies', height=0.8, source=bigram_source,\n                 line_color='white')\nbigram_plot.x_range.start = 0\nbigram_plot.ygrid.grid_line_color = None\n\n# Create a figure for the trigrams\ntrigram_plot = figure(y_range=trigram_labels, height=400, width=600,\n                      title='Top 10 most common trigrams in job titles')\ntrigram_plot.hbar(y='labels', right='frequencies', height=0.8, source=trigram_source,\n                  line_color='white')\n\ntrigram_plot.x_range.start = 0\ntrigram_plot.ygrid.grid_line_color = None\n\n# Show the plots\nshow(bigram_plot)\nshow(trigram_plot)\n"))),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"trigrams word cloud",src:n(2642).Z,width:"600",height:"400"}),"\n",(0,i.kt)("img",{alt:"trigrams word cloud",src:n(7411).Z,width:"600",height:"400"})),(0,i.kt)("p",null,"It seems that demand for ",(0,i.kt)("inlineCode",{parentName:"p"},"data analysts")," is even higher than ",(0,i.kt)("inlineCode",{parentName:"p"},"data scientists"),". Also, ",(0,i.kt)("inlineCode",{parentName:"p"},"senior")," level positions are pretty hot too, unlike ",(0,i.kt)("inlineCode",{parentName:"p"},"entry-level")," ones."),(0,i.kt)("p",null,"Image below is ",(0,i.kt)("strong",{parentName:"p"},"trigrams word cloud")," generated from this code:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"trigrams_freqs = {}\nfor tg in trigram_fdist.items():\n    trigrams_freqs[tg[0][0] + ' ' + tg[0][1] + ' ' + tg[0][2]] = tg[1]\n\nfrom wordcloud import WordCloud\n\n# Generate word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(trigrams_freqs)\n\n# Visualize word cloud\nplt.figure(figsize=(8, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"trigrams word cloud",src:n(5323).Z,width:"800",height:"400"})),(0,i.kt)("h3",{id:"identify-seniority-level"},"Identify Seniority Level"),(0,i.kt)("p",null,"We are going to analyze the job titles to identify different levels of seniority (e.g., junior, senior, manager) and see if there are any trends or patterns. Since we don't have a seniority level column in our dataset, we need to define some rules and keywords to identify which titles are senior level."),(0,i.kt)("p",null,"After eyeballing the dataset, I defined these keywords: ",(0,i.kt)("em",{parentName:"p"},"'senior', 'lead', 'principal', 'vp', 'director', 'staff', 'manager'"),". You can add more keywords to the list. Next, we check each title and if any of these keywords would be in the title, we assume that job title is a ",(0,i.kt)("inlineCode",{parentName:"p"},"Senior")," level job, ",(0,i.kt)("inlineCode",{parentName:"p"},"Other")," otherwise."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Define a function to identify seniority level based on keywords in the job title\ndef get_seniority_level(title):\n    senior_keywords = ['senior', 'lead', 'principal', 'vp', 'director', 'staff', 'manager']\n    for keyword in senior_keywords:\n        if keyword in title.lower():\n            return 'Senior'\n    return 'Other'\n\n# Apply the get_seniority_level function to the job title column and create a new seniority_level column\ndf['seniority_level'] = df['title'].apply(get_seniority_level)\n\n# Group the data by seniority level and count the number of job titles in each group\ngrouped_df = df.groupby('seniority_level')['title'].count().reset_index(name='count')\n\n# Create a Bokeh ColumnDataSource object for the bar chart\nsource = ColumnDataSource(grouped_df)\n\n# Create the bar chart with Bokeh\np = figure(x_range=grouped_df['seniority_level'], height=400, width=600,\n           title='Distribution of Job Titles by Seniority Level')\n\np.vbar(x='seniority_level', top='count', width=0.9, source=source)\np.xgrid.grid_line_color = None\np.y_range.start = 0\np.xaxis.axis_label = 'Seniority Level'\np.yaxis.axis_label = 'Number of Job Titles'\n\nshow(p)\n")),(0,i.kt)("p",null,"We can see that almost 30% of the total job titles are senior level. One interesting observation would be to compare this across multiple years and see the trend, if we had data from previous years."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"seniority level bar chart",src:n(4328).Z,width:"600",height:"400"})),(0,i.kt)("h2",{id:"topic-modeling-on-job-titles"},"Topic modeling on Job Titles"),(0,i.kt)("p",null,"Topic modeling is a natural language processing technique that can be used to identify topics or themes that are present in a large collection of text data. In the context of job titles, topic modeling can be used to identify the underlying topics or themes that are most common in data science job titles."),(0,i.kt)("p",null,"Here are the steps to perform topic modeling on job titles:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Clean and pre-process the job titles data, removing stop words, punctuation, and other non-relevant information."),(0,i.kt)("li",{parentName:"ol"},"Convert the preprocessed job titles data into a document-term matrix (DTM), which is a table that represents the frequency of each word in each job title."),(0,i.kt)("li",{parentName:"ol"},'Use a topic modeling algorithm, such as Latent Dirichlet Allocation (LDA), to identify the underlying topics or themes in the job titles data. The algorithm will identify the most common word combinations or "topics" that are present in the data.'),(0,i.kt)("li",{parentName:"ol"},'Inspect the resulting topics to understand what they represent and give them meaningful labels. For example, a topic could be labeled "Machine Learning Engineer" if it includes words like "machine learning", "engineer", and "data".'),(0,i.kt)("li",{parentName:"ol"},"Assign each job title to one or more topics based on the words it contains."),(0,i.kt)("li",{parentName:"ol"},"Analyze the distribution of topics across different job titles, companies, or other factors to identify patterns and trends in the data.\nBy using topic modeling on job titles, you can gain insights into the most common themes and subtopics that are present in data science job titles, which can help you understand the skills and qualifications that are most in demand in the data science job market.")),(0,i.kt)("p",null,"There are several libraries to do topic modeling including ",(0,i.kt)("a",{parentName:"p",href:"https://radimrehurek.com/gensim/index.html"},"Gensim")," and ",(0,i.kt)("a",{parentName:"p",href:"https://maartengr.github.io/BERTopic/index.html#visualize-topic-similarity"},"BERTopic"),". For this post we are using Gensim."),(0,i.kt)("p",null,"We're using NLTK to clean and tokenize job titles. Then, we create a Gensim ",(0,i.kt)("inlineCode",{parentName:"p"},"corpus"),", which is bag-of-words for titles. Then, we train an LDA model with different number of topics and choose the best one."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nfrom gensim.models.coherencemodel import CoherenceModel\n\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nstop_words = stopwords.words('english')\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess(text):\n    tokens = nltk.word_tokenize(text.lower())\n    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n    return tokens\n\n# Preprocess the job titles\ndf['title_tokens'] = df['title'].apply(preprocess)\n\n# Create a dictionary from the job titles\ndictionary = Dictionary(df['title_tokens'])\n\n# Create a corpus from the dictionary and job titles\ncorpus = [dictionary.doc2bow(title_tokens) for title_tokens in df['title_tokens']]\n\n# Train the LDA model\ntopics_range = [3,4,5,6,10]\nn_topics = 0\ncohs = []\nfor num_topics in topics_range:\n    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10, iterations=400)\n\n    cm = CoherenceModel(model=lda_model, texts=df['title_tokens'].values.tolist(), coherence='c_v')\n    # cm = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass')\n\n    coherence = cm.get_coherence()\n    cohs.append(coherence)\n    print(f\"Number of topics: {num_topics}, Coherence: {coherence}\")\n\n# Select the optimal number of topics\nn_topics = topics_range[np.argmax(cohs)]\nlda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topics, passes=10, iterations=500)\n")),(0,i.kt)("p",null,"By Computing the topic coherence, we can find out which number of topics is optimal. The bigger the coherence, the better the model is."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"Coherence: [0.5042733998871213, 0.4928157708829004, 0.5143670187831766, 0.5048272794509646, 0.47200016930420247]\n")),(0,i.kt)("p",null,"5 is the best number of topics in my experiment. You may get different number of topics as it may change for different iterations."),(0,i.kt)("p",null,"Let's see the top-10 words for each topic:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Print the top 10 words for each topic\nfor topic_id, topic_words in lda_model.show_topics(num_topics=n_topics, num_words=10, formatted=False):\n    print(f"Topic {topic_id}: {[word[0] for word in topic_words]}")\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"Topic 0: ['data', 'analyst', 'remote', 'sr', 'clinical', 'iii', 'scientist', 'lead', 'developer', 'hybrid']\nTopic 1: ['scientist', 'data', 'senior', 'analytics', 'level', 'staff', 'ii', 'entry', 'job', 'product']\nTopic 2: ['data', 'analyst', 'specialist', 'science', 'intern', 'associate', 'analytics', 'senior', 'director', 'hybrid']\nTopic 3: ['data', 'analyst', 'senior', 'analytics', 'business', 'manager', 'science', 'engineer', 'lead', 'health']\nTopic 4: ['data', 'analysis', 'analyst', 'scientist', 'program', 'science', 'machine', 'learning', 'business', 'lead']\n")),(0,i.kt)("p",null,"By looking at the top words of each topic we can manualy label the topics ",(0,i.kt)("em",{parentName:"p"},"subjectively"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Topic 0 label: data analyst"),(0,i.kt)("li",{parentName:"ul"},"Topic 1 label: data scientist"),(0,i.kt)("li",{parentName:"ul"},"Topic 2 label: entry level data analyst and analytics"),(0,i.kt)("li",{parentName:"ul"},"Topic 3 label: data analyst and busines analytics"),(0,i.kt)("li",{parentName:"ul"},"Topic 4 label: machine learning")),(0,i.kt)("p",null,"We are going to see the distribution of job titles by topics."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Get the topic distribution for each job title\ndf['topic_distribution'] = df['title_tokens'].apply(lambda x: lda_model[dictionary.doc2bow(x)])\n\n# Identify the most important topic for each job title\ndf['topic'] = df['topic_distribution'].apply(lambda x: np.argmax(x,axis=0)[1])\n\n# Group job titles by topic and count the number of titles in each group\ntopic_counts = df.groupby('topic')['title'].count()\n\n# Create the bar chart\n# Create a Bokeh data source with the topic counts\nsource = ColumnDataSource(data={\n    'topics': [str(t) for t in topic_counts.index],\n    'counts': topic_counts.values,\n})\n\n# Define the x-axis and y-axis ranges\nx_range = FactorRange(factors=source.data['topics'])\ny_range = (0, max(source.data['counts']) * 1.1)\n\n# Create a figure object\np = figure(x_range=x_range, y_range=y_range, height=400, width=800, title='Distribution of Job Titles by Topic')\n\n# Add a vertical bar chart to the figure\np.vbar(x='topics', top='counts', width=0.9, source=source, line_color='white', fill_color=Spectral5[0])\n\n# Set visual properties for the figure\np.xgrid.grid_line_color = None\np.xaxis.axis_label = 'Topics'\np.yaxis.axis_label = 'Number of Job Titles'\np.yaxis.minor_tick_line_color = None\np.yaxis.major_tick_line_color = None\n\n\nshow(p)\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"job title distributions",src:n(3278).Z,width:"800",height:"400"})),(0,i.kt)("p",null,"Image shows that topics of more than 50% of job titles either ",(0,i.kt)("inlineCode",{parentName:"p"},"topic 1")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"topic 3")," which are ",(0,i.kt)("inlineCode",{parentName:"p"},"data analyst")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"data scientist")," positions, respectively. Data analysts are even in higher demands according to our topics.\nWe can use ",(0,i.kt)("inlineCode",{parentName:"p"},"lda_model.get_topics()"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"lda_model.get_topic_terms()"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"lda_model.get_document_topics()")," to find topic distributions at word leve, get most important topic words and distribution of topics for each title. I use one of them to create a heatmap topic-title distributions and leave the rest as an exercise."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# create a heatmap of the topic distributions for each job title\nimport seaborn as sns\nimport numpy as np\n\ntopic_distributions = np.zeros((len(df), n_topics))\nfor i, doc in enumerate(corpus):\n    for topic, prob in lda_model.get_document_topics(doc):\n        topic_distributions[i][topic] = prob\n\nplt.figure(figsize=(5, 10))\nsns.heatmap(topic_distributions, cmap='Blues', cbar=True)\nplt.xlabel('Topic')\nplt.ylabel('Job Title Id')\nplt.title('Topic Distributions for Job Titles')\nplt.show()\n")),(0,i.kt)("p",null,"The image below shows the topic-title distributions for all the titles. The daarker the color, the higher weight that topic has for a title."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"seniority level bar chart",src:n(7672).Z,width:"500",height:"1000"})),(0,i.kt)("p",null,"There are several other interesting analyses that could be done. Following are just a few to name:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Run BERTopic and compare the results with LDA"),(0,i.kt)("li",{parentName:"ul"},'Job description analysis: Extracting salary, skills, education and "years of experience" from job description and find if there is any pattern between job titles and descriptions. For example, for extrating "years of experience", you can write a regex, however, you need to do multiple iterations to make sure you got the right data. Here\'s a good start for you:')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# Define the regex pattern to extract years of experience from the job description\nexp_regex = r'(\\d+)\\+? year[s]? ?(?:of )?experience'\n\n# Extract years of experience from the job description using the regex pattern\ndf['years_of_experience'] = df['description'].str.extract(exp_regex)\ndf['years_of_experience'] = df['years_of_experience'].fillna(0).astype(int)\n")),(0,i.kt)("p",null,"We have done quite a bit of data analysis. Next step would be to pull some of these visualizations together and build a dashboard, so users can interact with the data. That's going to be next part of this tutorial."),(0,i.kt)("p",null,"You can find the Jupyter notebook code ",(0,i.kt)("a",{parentName:"p",href:"/notebooks/05_ds_jobs_2023.html"},"here"),"."))}p.isMDXComponent=!0},2642:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/bigram-barchart-3706f61b795fa5163c57fa3b25916e16.png"},6953:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/eda-announcement-plot-e63fa8ed18eed983fc9ea6ed44ed9e96.png"},6349:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/eda-company-plot-0223bbd78ffd1bb45fa195b94ce050cb.png"},4350:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/eda-df-stat-7646192c36b6dc67f132df6a0a261469.png"},7745:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/eda-df-46b77cfec3456e33fe6d30a267ba2ceb.png"},7389:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/eda-title-length-plot-12f6fdcebfa7877af93d5476e87ecd05.png"},7136:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/eda-top20-words-b1412cd35e48a177b45b1dba7281ab1b.png"},3278:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/eda-topic-modeling-title-topic-dist-ec028914bb5f6abe77a29af7b2a4e8ad.png"},7672:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/eda-topic-modeling-topic-title-dist-0c4318037e37331dd45162d67510667a.png"},4328:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/seniority-level-plot-62a185bb566de55098205642863eb83b.png"},6633:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/top20-wordcloud-2a7dda58bf977698088c39759960901e.png"},7411:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/trigram-barchart-6212e65a3bcd97e2c2b3ccedb8413d40.png"},5323:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/trigrams-wordcloud-053239c824074797cf5612bf664d80d4.png"}}]);